{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ba5eb4b-bc0f-4ac3-b18f-bc0094f5b64d",
   "metadata": {},
   "source": [
    "# Script 1 - Access MUR and ERA5 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.patches as patches\n",
    "import datetime as dt\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.crs as ccrs\n",
    "import math\n",
    "import warnings \n",
    "warnings.simplefilter('ignore') \n",
    "import datetime as dt\n",
    "import datetime\n",
    "import geopandas as gpd\n",
    "import rioxarray\n",
    "from shapely.geometry import mapping\n",
    "import glob\n",
    "\n",
    "from cartopy.mpl.ticker import LongitudeFormatter, LatitudeFormatter\n",
    "from calendar import month_abbr\n",
    "from geopy import distance\n",
    "\n",
    "from harmony import BBox, Client, Collection, Request, Environment, LinkType "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a477835",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read in and make a map of the masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47156a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open California mask\n",
    "cali = xr.open_dataset('../data/masks/mask_California_MURSST.nc')\n",
    "cali.close()\n",
    "\n",
    "#open Benguela mask\n",
    "beng = xr.open_dataset('../data/masks/mask_Benguela_MURSST.nc')\n",
    "beng.close()\n",
    "\n",
    "#open Humboldt mask\n",
    "hum = xr.open_dataset('../data/masks/mask_Humboldt_MURSST.nc')\n",
    "hum.close()\n",
    "\n",
    "#open Iberian-Canary mask\n",
    "iber = xr.open_dataset('../data/masks/mask_Iberian-Canary_MURSST.nc')\n",
    "iber.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ad501c-16d8-4b79-9009-bf1104afca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create copies of extending each by 100 km\n",
    "cali_df = iber.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_100 = iber.copy()\n",
    "cali_100['lon'] = cali_100['lon'] - 1\n",
    "cali_100 = cali_100.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_200 = iber.copy()\n",
    "cali_200['lon'] = cali_200['lon'] - 2\n",
    "cali_200 = cali_200.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_300 = iber.copy()\n",
    "cali_300['lon'] = cali_300['lon'] - 3\n",
    "cali_300 = cali_300.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_400 = iber.copy()\n",
    "cali_400['lon'] = cali_400['lon'] - 4\n",
    "cali_400 = cali_400.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_500 = iber.copy()\n",
    "cali_500['lon'] = cali_500['lon'] - 5\n",
    "cali_500 = cali_500.to_dataframe().reset_index().dropna()\n",
    "\n",
    "cali_600 = iber.copy()\n",
    "cali_600['lon'] = cali_600['lon'] - 6\n",
    "cali_600 = cali_600.to_dataframe().reset_index().dropna()\n",
    "\n",
    "new_cali = pd.concat([cali_df, cali_100])\n",
    "new_cali = pd.concat([new_cali, cali_200])\n",
    "new_cali = pd.concat([new_cali, cali_300])\n",
    "new_cali = pd.concat([new_cali, cali_400])\n",
    "new_cali = pd.concat([new_cali, cali_500])\n",
    "new_cali = pd.concat([new_cali, cali_600]).set_index(['lat', 'lon'])\n",
    "new_cali = new_cali.groupby(level=new_cali.index.names).mean().reset_index()\n",
    "# new_cali.loc[(new_cali['lat'] >= 34) & (new_cali['lat'] <= 46) & \n",
    "#              (new_cali['lon'] >= -128.5) & (new_cali['lon'] <= -127.5), 'analysed_sst'] = 1\n",
    "\n",
    "# new_cali = new_cali.fillna(100)\n",
    "\n",
    "new_cali.set_index(['lat', 'lon']).to_xarray().to_netcdf('../data/masks/mask_Iberian_MURSST_extendedSubregion.nc')\n",
    "new_cali.set_index(['lat', 'lon']).to_xarray().analysed_sst.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbfec37-6303-4fc0-919a-50f85f772430",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Download ZARR Data for Extended Subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed7a696-e38d-4c02-ab21-5929f61229c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Download mur data\n",
    "#open zarr\n",
    "mur = xr.open_zarr('https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1')\n",
    "#resample to make monthly\n",
    "mur = mur.resample(time = '1MS').mean()\n",
    "mur = mur.sel(time = ~mur.time.dt.year.isin([2020]))\n",
    "\n",
    "#open shapefile \n",
    "shp = gpd.read_file('../data/masks/EBUS_Shapefiles/Iberian/Iberian_Mask_EBUS_shp.shp')\n",
    "\n",
    "#prepare xarray data to be clipped\n",
    "mur.rio.set_spatial_dims(x_dim = \"lon\", y_dim = \"lat\", inplace = True)\n",
    "mur.rio.write_crs(\"epsg:4326\", inplace = True)\n",
    "\n",
    "#clip the data\n",
    "mur_clipped = mur.rio.clip(shp.geometry.apply(mapping), shp.crs, drop = True)\n",
    "\n",
    "# save the clipped data\n",
    "# mur_clipped.to_netcdf('/Volumes/GoogleDrive/Shared drives/Commons/STAFF_folders/Marisol/Gammon-Sol/MURR/Humboldt/Extended_Subregion/Hum_ExtendedSubregions_ZARR.nc')\n",
    "mur_clipped.analysed_sst.to_netcdf('../data/Iber_ExtendedSubregions_ZARR.nc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fd7004-d787-47d2-b4ad-956e6a0e7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mur = xr.open_dataset('../data/Iber_ExtendedSubregions_ZARR.nc')\n",
    "mur.close()\n",
    "\n",
    "mur = mur.where(~np.isnan(mur.analysed_sst), drop = True)\n",
    "\n",
    "mur = mur.drop('spatial_ref')\n",
    "\n",
    "mur['analysed_sst'] = mur['analysed_sst'] - 273.15\n",
    "\n",
    "#replace / in the data variables if necessary and encode the data\n",
    "encoding = {}\n",
    "for variable in mur.data_vars:\n",
    "    #temp_clip = temp_clip.rename({variable: variable.replace('/_', '')})\n",
    "    encoding[variable] = {'dtype': 'float32', 'scale_factor': 0.1, '_FillValue': -9999}\n",
    "\n",
    "#drop the missing MPAs and save with encoding variable\n",
    "mur.to_netcdf('../data/Iber_ExtendedSubregions_ZARR_processed.nc', encoding = encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6207f61-089b-4fb8-ad83-0995a8a91b45",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Harmony Job for MUR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59fc725-884a-4392-94e3-1a492b2f4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of coordinates for each EBUS/subregion\n",
    "cali_lats = np.array([[28.5,34.5],[34.5,40.4],[40.4,46]]) \n",
    "cali_lons = np.array([[-130, -114],[-134.5, -120], [-134.5, -123]])\n",
    "hum_lats = np.array([[-42,-28],[-28,-17],[-17,-10]]) \n",
    "hum_lons = np.array([[-79,-69],[-76,-69],[-81,-72]]) \n",
    "iber_lats = np.array([[15,21.33],[21.33,30],[37, 43.39]]) \n",
    "iber_lons = np.array([[-21,-16],[-21,-9],[-14,-7]]) \n",
    "beng_lats = np.array([[-34.8,-28.63],[-28.63, -22],[-22,-15]]) \n",
    "beng_lons = np.array([[13, 20],[10, 17],[8, 15]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa3b5c2-50df-47f1-b34f-6f33a682e5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the shapefile\n",
    "shp = gpd.read_file('../data/masks/EBUS_Shapefiles/Iberian/Iberian_Mask_EBUS_shp.shp')\n",
    "shp.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b30ce-6483-4778-bd3e-0027f9fccd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the Harmony Client.\n",
    "harmony_client = Client(env=Environment.PROD)\n",
    "\n",
    "#specify the data\n",
    "collection = Collection(id='MUR-JPL-L4-GLOB-v4.1')\n",
    "\n",
    "#specify start and stop times\n",
    "start_day = datetime.datetime(2020,1,1,0,0,0)\n",
    "end_day = datetime.datetime(2022,5,31,0,0,0)\n",
    "\n",
    "#create a request \n",
    "request = Request(\n",
    "    collection=collection,\n",
    "    temporal={\n",
    "        'start': start_day,\n",
    "        'stop': end_day\n",
    "    },\n",
    "    spatial=BBox(shp.total_bounds[0], shp.total_bounds[1], shp.total_bounds[2], shp.total_bounds[3]), #total boundary based on shapefile\n",
    "    variables=['analysed_sst'],\n",
    "    # granule_id=granuleIDs,\n",
    "    # concatenate = True,\n",
    ")\n",
    "\n",
    "request.is_valid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549d9033-f826-498f-8b32-06f1de8398fb",
   "metadata": {},
   "source": [
    "Job IDs:\n",
    "* California: a05723f8-92d7-49d3-ac67-4256fa70b2fa\n",
    "* Benguela: fd3df6f8-890c-4f4c-bc12-b32fcdac396f\n",
    "* Humboldt: 2a5ceec4-dd89-4759-ae9d-b372e696507e\n",
    "* Iberian-Canary: b2172a9b-8820-418a-be38-fb609308c334"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69be6543-b030-414c-86c4-0a61b3e0943c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(harmony_client.request_as_curl(request))\n",
    "job_id = harmony_client.submit(request)\n",
    "print(f'Job ID: {job_id}') # This job id is shareable:show how to do this\n",
    "response = harmony_client.result_json(job_id, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b1b4f-8c56-4776-b2bd-b073b29d08a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmony_client.resume(job_id)\n",
    "#harmony_client.status(job_id) \n",
    "harmony_client.wait_for_processing(job_id, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f766b0d9-f217-4ebb-a53d-1d02b43581ce",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#link to directory to download data\n",
    "down_dir = '../data/data_download/'\n",
    "\n",
    "#download the files\n",
    "futures = harmony_client.download_all(job_id, directory=down_dir, overwrite=False)\n",
    "file_names = [f.result() for f in futures]\n",
    "sorted(file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde06c0e-8836-4efb-ae4e-12dfcf7c11c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Combine the ZARR and Cloud Data\n",
    "long_name = 'Iberian'\n",
    "short_name = 'Iber'\n",
    "\n",
    "#open cloud data\n",
    "files = glob.glob('../data/data_download/*.nc4')\n",
    "files.sort()\n",
    "\n",
    "mur_cloud = xr.open_mfdataset(files, engine = 'netcdf4')\n",
    "mur_cloud.close()\n",
    "print('opened all files')\n",
    "\n",
    "#resample monthly\n",
    "mur_cloud = mur_cloud.resample(time = '1MS').mean()\n",
    "\n",
    "#open shapefile \n",
    "shp = gpd.read_file('../data/masks/EBUS_Shapefiles/'+long_name+'/'+long_name+'_Mask_EBUS_shp.shp')\n",
    "\n",
    "#prepare xarray data to be clipped\n",
    "mur_cloud.rio.set_spatial_dims(x_dim = \"lon\", y_dim = \"lat\", inplace = True)\n",
    "mur_cloud.rio.write_crs(\"epsg:4326\", inplace = True)\n",
    "\n",
    "#clip the cloud data\n",
    "mur_cloud = mur_cloud.rio.clip(shp.geometry.apply(mapping), shp.crs, drop = True)\n",
    "print('clipped data')\n",
    "\n",
    "#remove spatial reference\n",
    "mur_cloud = mur_cloud.drop('spatial_ref')\n",
    "\n",
    "#convert K to C\n",
    "mur_cloud['analysed_sst'] = mur_cloud['analysed_sst'] - 273.15\n",
    "print('converted temperature')\n",
    "\n",
    "#encode the data to save space\n",
    "encoding = {}\n",
    "for variable in mur_cloud.data_vars:\n",
    "    #temp_clip = temp_clip.rename({variable: variable.replace('/_', '')})\n",
    "    encoding[variable] = {'dtype': 'float32', 'scale_factor': 0.1, '_FillValue': -9999}\n",
    "\n",
    "#save the clipped cloud data\n",
    "mur_cloud.to_netcdf('../data/'+short_name+'_ExtendedSubregions_Cloud.nc', encoding = encoding)\n",
    "print('saved cloud file')\n",
    "\n",
    "#open zarr data\n",
    "mur_zarr = xr.open_dataset('../data/'+short_name+'_ExtendedSubregions_ZARR.nc')\n",
    "mur_zarr.close()\n",
    "\n",
    "#join data together\n",
    "mur = xr.merge([mur_zarr, mur_cloud])\n",
    "print('merged zarr and cloud files')\n",
    "\n",
    "#save data\n",
    "mur.to_netcdf('../data/'+short_name+'_ExtendedSubregions_All.nc')\n",
    "print('saved merged file')\n",
    "mur"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430addaf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read in MURR data and Filter it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db244139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the file name using, in the format:\n",
    "# the s3 bucket [mur-sst], and the region [us-west-2], and the folder if applicable [zarr-v1] \n",
    "file_location = 'https://mur-sst.s3.us-west-2.amazonaws.com/zarr-v1'\n",
    "\n",
    "ds_sst = xr.open_zarr(file_location,consolidated=True) # open a zarr file using xarray\n",
    "# it is similar to open_dataset but it only reads the metadata\n",
    "\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc95bfd-e27f-43c5-bdcc-90bb0a9b5283",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_url = \"https://thredds.jpl.nasa.gov/thredds/dodsC/OceanTemperature/MUR-JPL-L4-GLOB-v4.1.nc\"\n",
    "ds_sst = xr.open_dataset(aggr_url)\n",
    "ds_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e348d13-9bb2-42c1-9584-a0323b9f67cf",
   "metadata": {},
   "source": [
    "#### Humbolt Data\n",
    "\n",
    "##### Thrreds data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd67047-0f11-4936-ad76-6d83a4c899a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time range. data range available: 2002-06-01 to 2020-01-20. [start with a short period]\n",
    "dater = ['2022-06-01','2022-06-30'] # dates on the format 'YYYY-MM-DD' as string\n",
    "latr = [np.nanmin(hum.lat), np.nanmax(hum.lat)]\n",
    "lonr = [np.nanmin(hum.lon), np.nanmax(hum.lon)]\n",
    "\n",
    "#remove all values that are for lakes\n",
    "sst = ds_sst.analysed_sst.sel(time = slice(dater[0],dater[1]), lat = slice(latr[0],latr[1]), lon = slice(lonr[0],lonr[1]))\n",
    "\n",
    "sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "sst.attrs['units']='deg C' #add attributes to the data\n",
    "sst.to_netcdf('../data/data_download/Humboldt/20220601-20220630-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04e9ea4-1d12-4615-820b-4d94e182495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_20 = xr.open_mfdataset('../data/data_download/Humboldt/2020*.nc'.format(dir))\n",
    "sst_20.close()\n",
    "\n",
    "#remove all values that are for lakes\n",
    "#sst_filtered = sst_20.where(sst_20.mask != 5, np.nan)\n",
    "\n",
    "#mask the MURR data\n",
    "sst_hum = sst_20*hum.analysed_sst\n",
    "\n",
    "#filter the data using time from above and take the monthly means\n",
    "sst = sst_hum.resample(time = 'M').mean(dim={'time'}, skipna=True, keep_attrs=True)\n",
    "\n",
    "sst.to_netcdf('../data/data_download/Humbolt/hum_MURR_2020.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86846f99-174d-42dd-b194-af280d72a72c",
   "metadata": {},
   "source": [
    "##### ZARR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dcbc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "year = input(\"Input year (03-21):\")\n",
    "\n",
    "# time range. data range available: 2002-06-01 to 2020-01-20. [start with a short period]\n",
    "dater = ['20'+year+'-01-01','20'+year+'-12-31'] # dates on the format 'YYYY-MM-DD' as string\n",
    "\n",
    "#remove all values that are for lakes\n",
    "sst_filtered = ds_sst.where(ds_sst.mask != 5, np.nan)\n",
    "\n",
    "#mask the MURR data\n",
    "sst_hum = sst_filtered*hum.analysed_sst\n",
    "\n",
    "#filter the data using time from above and take the monthly means\n",
    "sst = sst_hum['analysed_sst'].sel(time = slice(dater[0],dater[1])).resample(time = 'M').mean(dim={'time'}, skipna=True, keep_attrs=True)\n",
    "\n",
    "sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "sst.attrs['units']='deg C' #add attributes to the data\n",
    "sst\n",
    "#sst.to_netcdf('..data/data_download/Humbolt/hum_MURR_20'+year+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e895048-6316-4693-ac1c-6ac7d4eca966",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### California Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3029e0ad-2e7c-45e3-a8e2-cb3ac789ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time range. data range available: 2002-06-01 to 2020-01-20. [start with a short period]\n",
    "dater = ['2022-05-01','2022-05-31'] # dates on the format 'YYYY-MM-DD' as string\n",
    "latr = [np.nanmin(cali.lat), np.nanmax(cali.lat)]\n",
    "lonr = [np.nanmin(cali.lon), np.nanmax(cali.lon)]\n",
    "\n",
    "#remove all values that are for lakes\n",
    "sst = ds_sst.analysed_sst.sel(time = slice(dater[0],dater[1]), lat = slice(latr[0],latr[1]), lon = slice(lonr[0],lonr[1]))\n",
    "\n",
    "sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "sst.attrs['units']='deg C' #add attributes to the data\n",
    "sst.to_netcdf('../data/data_download/California/20220501-20220531-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27599b-c257-492a-adfd-951fb72e1637",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_20 = xr.open_mfdataset('../data/data_download/California/2022*.nc'.format(dir))\n",
    "sst_20.close()\n",
    "\n",
    "#remove all values that are for lakes\n",
    "#sst_filtered = sst_20.where(sst_20.mask != 5, np.nan)\n",
    "\n",
    "#mask the MURR data\n",
    "sst_hum = sst_20*cali.analysed_sst\n",
    "\n",
    "#filter the data using time from above and take the monthly means\n",
    "sst = sst_hum.resample(time = 'M').mean(dim={'time'}, skipna=True, keep_attrs=True)\n",
    "\n",
    "sst.to_netcdf('..data/data_download/California/cali_MURR_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe07229c-6aa1-4830-8e69-3091ecac4dc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iberian-Canary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f270b054-5828-4cc2-b705-62fd73e301e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2002,2023):\n",
    "\n",
    "    year = str(i)\n",
    "\n",
    "    if year == '2002': #if it's the first year of MURR\n",
    "        dates = [[year+'-06-01',year+'-06-30'],\n",
    "                 [year+'-07-01',year+'-07-31'],\n",
    "                 [year+'-08-01',year+'-08-31'],\n",
    "                 [year+'-09-01',year+'-09-30'],\n",
    "                 [year+'-10-01',year+'-10-31'],\n",
    "                 [year+'-11-01',year+'-11-30'],\n",
    "                 [year+'-12-01',year+'-12-31']]\n",
    "    elif year == '2022': #if it's the most recent year of MURR\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-28'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31']]\n",
    "    elif year in ['2004','2008','2012','2016','2020']: #if it's a leap year\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-29'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31'],\n",
    "                [year+'-06-01',year+'-06-30'],\n",
    "                [year+'-07-01',year+'-07-31'],\n",
    "                [year+'-08-01',year+'-08-31'],\n",
    "                [year+'-09-01',year+'-09-30'],\n",
    "                [year+'-10-01',year+'-10-31'],\n",
    "                [year+'-11-01',year+'-11-30'],\n",
    "                [year+'-12-01',year+'-12-31']]\n",
    "    else: #if it's any other year\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-28'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31'],\n",
    "                [year+'-06-01',year+'-06-30'],\n",
    "                [year+'-07-01',year+'-07-31'],\n",
    "                [year+'-08-01',year+'-08-31'],\n",
    "                [year+'-09-01',year+'-09-30'],\n",
    "                [year+'-10-01',year+'-10-31'],\n",
    "                [year+'-11-01',year+'-11-30'],\n",
    "                [year+'-12-01',year+'-12-31']]\n",
    "\n",
    "    for j in range(12):\n",
    "        # time range. data range available: 2002-06-01 to 2022-06-30. [start with a short period]\n",
    "        dater = dates[j] # dates on the format 'YYYY-MM-DD' as string\n",
    "        latr = [np.nanmin(iber.lat), np.nanmax(iber.lat)]\n",
    "        lonr = [np.nanmin(iber.lon), np.nanmax(iber.lon)]\n",
    "\n",
    "        start_date = ''.join(dates[j][0].split('-'))\n",
    "        end_date = ''.join(dates[j][1].split('-'))\n",
    "\n",
    "        #remove all values that are for lakes\n",
    "        sst = ds_sst.analysed_sst.sel(time = slice(dater[0],dater[1]), lat = slice(latr[0],latr[1]), lon = slice(lonr[0],lonr[1]))\n",
    "\n",
    "        sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "        sst.attrs['units']='deg C' #add attributes to the data\n",
    "        sst.to_netcdf('../data/data_download/Iberian-Canary/'+start_date+'-'+end_date+'-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81130604-b248-4387-8637-92b28eb2860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_20 = xr.open_mfdataset('../data/data_download/Iberian-Canary/2022*.nc'.format(dir))\n",
    "sst_20.close()\n",
    "\n",
    "#remove all values that are for lakes\n",
    "#sst_filtered = sst_20.where(sst_20.mask != 5, np.nan)\n",
    "\n",
    "#mask the MURR data\n",
    "sst_hum = sst_20*iber.analysed_sst\n",
    "\n",
    "#filter the data using time from above and take the monthly means\n",
    "sst = sst_hum.resample(time = 'M').mean(dim={'time'}, skipna=True, keep_attrs=True)\n",
    "\n",
    "sst.to_netcdf('../data/data_download/Iberian-Canary/iber_MURR_2022.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aafa98b-f935-4680-b8cb-70b576b6641e",
   "metadata": {},
   "source": [
    "Benguela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29237ca8-4537-4e74-90c1-bdbbb7d2c160",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [2022]:\n",
    "\n",
    "    year = str(i)\n",
    "\n",
    "    if year == '2002': #if it's the first year of MURR\n",
    "        dates = [[year+'-06-01',year+'-06-30'],\n",
    "                 [year+'-07-01',year+'-07-31'],\n",
    "                 [year+'-08-01',year+'-08-31'],\n",
    "                 [year+'-09-01',year+'-09-30'],\n",
    "                 [year+'-10-01',year+'-10-31'],\n",
    "                 [year+'-11-01',year+'-11-30'],\n",
    "                 [year+'-12-01',year+'-12-31']]\n",
    "    elif year == '2022': #if it's the most recent year of MURR\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-28'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31']]\n",
    "    elif year in ['2004','2008','2012','2016','2020']: #if it's a leap year\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-29'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31'],\n",
    "                [year+'-06-01',year+'-06-30'],\n",
    "                [year+'-07-01',year+'-07-31'],\n",
    "                [year+'-08-01',year+'-08-31'],\n",
    "                [year+'-09-01',year+'-09-30'],\n",
    "                [year+'-10-01',year+'-10-31'],\n",
    "                [year+'-11-01',year+'-11-30'],\n",
    "                [year+'-12-01',year+'-12-31']]\n",
    "    else: #if it's any other year\n",
    "        dates = [[year+'-01-01',year+'-01-31'],\n",
    "                [year+'-02-01',year+'-02-28'],\n",
    "                [year+'-03-01',year+'-03-31'],\n",
    "                [year+'-04-01',year+'-04-30'],\n",
    "                [year+'-05-01',year+'-05-31'],\n",
    "                [year+'-06-01',year+'-06-30'],\n",
    "                [year+'-07-01',year+'-07-31'],\n",
    "                [year+'-08-01',year+'-08-31'],\n",
    "                [year+'-09-01',year+'-09-30'],\n",
    "                [year+'-10-01',year+'-10-31'],\n",
    "                [year+'-11-01',year+'-11-30'],\n",
    "                [year+'-12-01',year+'-12-31']]\n",
    "\n",
    "    for j in range(12):\n",
    "        # time range. data range available: 2002-06-01 to 2022-06-30. [start with a short period]\n",
    "        dater = dates[j] # dates on the format 'YYYY-MM-DD' as string\n",
    "        latr = [np.nanmin(beng.lat), np.nanmax(beng.lat)]\n",
    "        lonr = [np.nanmin(beng.lon), np.nanmax(beng.lon)]\n",
    "\n",
    "        start_date = ''.join(dates[j][0].split('-'))\n",
    "        end_date = ''.join(dates[j][1].split('-'))\n",
    "\n",
    "        #remove all values that are for lakes\n",
    "        sst = ds_sst.analysed_sst.sel(time = slice(dater[0],dater[1]), lat = slice(latr[0],latr[1]), lon = slice(lonr[0],lonr[1]))\n",
    "\n",
    "        sst = sst-273.15 # transform units from Kelvin to Celsius\n",
    "        sst.attrs['units']='deg C' #add attributes to the data\n",
    "        sst.to_netcdf('../data/data_download/Benguela/'+start_date+'-'+end_date+'-JPL-L4_GHRSST-SSTfnd-MUR-GLOB-v02.0-fv04.1.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319624e2-08e6-4ef1-98d7-b1385382f657",
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_20 = xr.open_mfdataset('../data/data_download/Benguela/2022*.nc'.format(dir))\n",
    "sst_20.close()\n",
    "\n",
    "#remove all values that are for lakes\n",
    "#sst_filtered = sst_20.where(sst_20.mask != 5, np.nan)\n",
    "\n",
    "#mask the MURR data\n",
    "sst_hum = sst_20*beng.analysed_sst\n",
    "\n",
    "#filter the data using time from above and take the monthly means\n",
    "sst = sst_hum.resample(time = 'M').mean(dim={'time'}, skipna=True, keep_attrs=True)\n",
    "\n",
    "sst.to_netcdf('../data/data_download/Bengula/beng_MURR_2022.nc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dcd10c-670f-4362-b128-72b51cca90fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ERA5 Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b55ef4-e38a-4427-93c5-53ecfc63173f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#open Benguela mask\n",
    "mask = xr.open_dataset('../data/mask/mask_Benguela_MURSST.nc')\n",
    "mask.close()\n",
    "#ebus = 'benguela'\n",
    "\n",
    "if ebus != 'benguela':\n",
    "    mask['lon'] = mask.lon + 360\n",
    "\n",
    "#create empty xarrays to store output\n",
    "vds_merged = xr.Dataset()\n",
    "sst_ds_merged = xr.Dataset()\n",
    "iyr = 2002 \n",
    "fyr = 2022\n",
    "\n",
    "#loop through each year and month to access the ERA5 data \n",
    "for iy, y in enumerate(range(iyr, fyr+1)): # for loop over the selected years\n",
    "    \n",
    "    #if the year is 2002\n",
    "    if y == 2002:\n",
    "        \n",
    "        for i in range(6,13): # for loop for each month\n",
    "            mon = i\n",
    "\n",
    "            ###acquiring meridional wind v10m\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/northward_wind_at_10_metres.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True) # open access to data\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "            \n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era  = mask.analysed_sst.interp_like(ds.northward_wind_at_10_metres.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            vds = ds_subset.groupby('time0.date').mean().load() \n",
    "            vds['date'] = pd.DatetimeIndex(vds['date'].values)\n",
    "            vds = vds.rename({'date':'time0'})\n",
    "            vds_merged = xr.merge([vds_merged, vds])\n",
    "\n",
    "            ###acquiring sea surface temperature\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/sea_surface_temperature.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True)\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "\n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era  = mask.analysed_sst.interp_like(ds.sea_surface_temperature.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            sst_ds = ds_subset.groupby('time0.date').mean().load() \n",
    "            sst_ds['date'] = pd.DatetimeIndex(sst_ds['date'].values)\n",
    "            sst_ds = sst_ds.rename({'date':'time0'})\n",
    "            sst_ds_merged = xr.merge([sst_ds_merged, sst_ds]) \n",
    "            \n",
    "    #if the year is 2022        \n",
    "    elif y == 2022: \n",
    "        for i in range(1,6): # for loop for each month\n",
    "            mon = i\n",
    "            \n",
    "            ###acquiring meridional wind v10m\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/northward_wind_at_10_metres.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True) # open access to data\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "            \n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era  = mask.analysed_sst.interp_like(ds.northward_wind_at_10_metres.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            vds = ds_subset.groupby('time0.date').mean().load() \n",
    "            vds['date'] = pd.DatetimeIndex(vds['date'].values)\n",
    "            vds = vds.rename({'date':'time0'})\n",
    "            vds_merged = xr.merge([vds_merged, vds])\n",
    "\n",
    "            ###acquiring sea surface temperature\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/sea_surface_temperature.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True)\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "\n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era  = mask.analysed_sst.interp_like(ds.sea_surface_temperature.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            sst_ds = ds_subset.groupby('time0.date').mean().load() \n",
    "            sst_ds['date'] = pd.DatetimeIndex(sst_ds['date'].values)\n",
    "            sst_ds = sst_ds.rename({'date':'time0'})\n",
    "            sst_ds_merged = xr.merge([sst_ds_merged, sst_ds]) \n",
    "\n",
    "    else: \n",
    "        for i in range(1,13): # for loop for each month\n",
    "            mon = i\n",
    "            \n",
    "            ###acquiring meridional wind v10m\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/northward_wind_at_10_metres.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True) # open access to data\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "            \n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era = mask.analysed_sst.interp_like(ds.northward_wind_at_10_metres.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            vds = ds_subset.groupby('time0.date').mean().load() \n",
    "            vds['date'] = pd.DatetimeIndex(vds['date'].values)\n",
    "            vds = vds.rename({'date':'time0'})\n",
    "            vds_merged = xr.merge([vds_merged, vds])\n",
    "\n",
    "            ###acquiring sea surface temperature\n",
    "            #open data\n",
    "            file_location = 'https://era5-pds.s3.us-east-1.amazonaws.com/zarr/'+str(y)+'/'+str(mon).zfill(2)+'/data/sea_surface_temperature.zarr'\n",
    "            ds = xr.open_zarr(file_location,consolidated=True)\n",
    "            ds = ds.sortby('time0')\n",
    "            ds = ds.sel(lat = slice(mask.analysed_sst.lat.max(), mask.analysed_sst.lat.min()), lon = slice(mask.analysed_sst.lon.min(), mask.analysed_sst.lon.max()))\n",
    "\n",
    "            #upsample the mask to match the era5 data\n",
    "            mask_era  = mask.analysed_sst.interp_like(ds.sea_surface_temperature.isel(time0 = 0))\n",
    "\n",
    "            #mask the monthly era5 data and calculate the daily mean\n",
    "            ds_subset = mask_era * ds\n",
    "            sst_ds = ds_subset.groupby('time0.date').mean().load() \n",
    "            sst_ds['date'] = pd.DatetimeIndex(sst_ds['date'].values)\n",
    "            sst_ds = sst_ds.rename({'date':'time0'})\n",
    "            sst_ds_merged = xr.merge([sst_ds_merged, sst_ds]) \n",
    "\n",
    "# calculate to v wind stress\n",
    "ra = 1.225\n",
    "cd_v = xr.where(vds_merged['northward_wind_at_10_metres'] < 11, 0.49+0.065*abs(vds_merged['northward_wind_at_10_metres']), 1.2)\n",
    "cd_v = cd_v/1000\n",
    "vds_merged['northward_wind_stress']= cd_v * ra * vds_merged['northward_wind_at_10_metres']        \n",
    "\n",
    "#merge all datasets into one xarray file\n",
    "era5 = xr.merge([vds_merged, sst_ds_merged])\n",
    "\n",
    "#take the monthly average\n",
    "era5 = era5.resample(time0 = 'M').mean()\n",
    "\n",
    "#save file\n",
    "era5.to_netcdf('../data/data_download/ERA5_Chile/Benguela_ERA5.nc') #save data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092fbff-dcaf-4b72-ae89-1672a25b1805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
